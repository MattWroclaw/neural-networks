{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJ+rERlo8cpZSixh/kPGgr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MattWroclaw/neural-networks/blob/main/07_rnn/02_text_classifier_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NPu0DAUqOxB"
      },
      "outputs": [],
      "source": [
        "# Przygotowanie środowiska do pracy z Tensorflow 2.0.\n",
        "# Jeśli otrzymasz błąd podczas instalacji Tensorflow uruchom tę komórkę raz jeszcze.\n",
        "\n",
        "# !pip uninstall -y tensorflow\n",
        "# !pip install -q tensorflow==2.0.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten"
      ],
      "metadata": {
        "id": "cF6wjD8GqZx7"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://storage.googleapis.com/esmartdata-courses-files/ann-course/reviews.zip\n",
        "!unzip -q reviews.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dLnx6nrqb_k",
        "outputId": "a875c1bc-e7b3-4e7c-de4a-29e742847ab0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-23 18:36:44--  https://storage.googleapis.com/esmartdata-courses-files/ann-course/reviews.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.157.207, 142.251.8.207, 142.251.170.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.157.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42878657 (41M) [application/x-zip-compressed]\n",
            "Saving to: ‘reviews.zip’\n",
            "\n",
            "reviews.zip         100%[===================>]  40.89M  12.4MB/s    in 4.8s    \n",
            "\n",
            "2024-10-23 18:36:51 (8.50 MB/s) - ‘reviews.zip’ saved [42878657/42878657]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = './reviews'\n",
        "train_dir = os.path.join(data_dir, 'train')\n",
        "\n",
        "train_texts = []\n",
        "train_labels = []\n",
        "\n",
        "for label_type in ['neg', 'pos']:\n",
        "    dir_name = os.path.join(train_dir, label_type)\n",
        "    for fname in os.listdir(dir_name):\n",
        "        if fname[-4:] == '.txt':\n",
        "            f = open(os.path.join(dir_name, fname))\n",
        "            train_texts.append(f.read())\n",
        "            f.close()\n",
        "            if label_type == 'neg':\n",
        "                train_labels.append(0)\n",
        "            else:\n",
        "                train_labels.append(1)"
      ],
      "metadata": {
        "id": "RMYjZ4uBrTxK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dir = os.path.join(data_dir, 'test')\n",
        "\n",
        "test_texts = []\n",
        "test_labels = []\n",
        "\n",
        "for label_type in ['neg', 'pos']:\n",
        "    dir_name = os.path.join(test_dir, label_type)\n",
        "    for fname in os.listdir(dir_name):\n",
        "        if fname[-4:] == '.txt':\n",
        "            f = open(os.path.join(dir_name, fname))\n",
        "            test_texts.append(f.read())\n",
        "            f.close()\n",
        "            if label_type == 'neg':\n",
        "                test_labels.append(0)\n",
        "            else:\n",
        "                test_labels.append(1)"
      ],
      "metadata": {
        "id": "s1sXVHvZrVFN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrhHOXSRsYmp",
        "outputId": "711f1a10-3cfc-416c-ff9c-f5b1657c8768"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Bernard Rapp passed away last year and was a very cultured journalist. Cinema was one of his biggest passions (he penned a vast worldwide dictionary of films) and so he was bound to wield a camera at least one time in his life. But the films he left garnered lukewarm reviews: \"Tiré à Part\" (1996) in spite of Terence Stamp\\'s sensational performance was very caricatured in the depiction of the characters, \"une Affaire De Goût\" (2000) was a slick affair even if Bernard Giraudeau delivered a perverse performance, \"Pas Si Grave\" (2003) was another let-down and \"un Petit Jeu Sans Conséquence\" is as underwhelming as its predecessors. Its comic potential is exploited in a flimsy way.<br /><br />And however, the starting idea let predict a twirling, spiritual comedy. A couple held by Yvan Attal and Sandrine Kiberlain who invited their friends is in full moving in a lascivious mansion. To play with their guests, they pretend to part company with each other. And things don\\'t go as planned because the announcement of their separation doesn\\'t surprise them. The two lovers start to ponder about the validity of their couple.<br /><br />In spite of lush scenery and the promising material he had at his disposal, Rapp\\'s undistinguished directing can\\'t manage to give life to this game with unexpected consequences. The plot follows a well-worn pattern with characters who have specific well-known functions and masks that are unveiled about who they really are. Verbal or situation comic effects often fall flat. A bad editing fades a little more the film with this bad habit from Rapp to abruptly cut many sequences. Even the actors\\' sincere input in the venture is debatable. They seem to be bored and to recite their texts than to live them, especially Sandrine Kiberlain. The audience is soon caught in a deep torpor.<br /><br />It\\'s regrettable to say it: Bernard Rapp\\'s films never lived up to his intentions as \"un Petit Jeu sans Conséquence\" bears witness.',\n",
              " '\"Sky Captain\" may be considered an homage to comic books, pulp adventures and movie serials but it contains little of the magic of some of the best from those genres. One contributor says that enjoyment of the film depends on whether or not one recognizes the films influences. I don\\'t think this is at all true. One\\'s expectations of the films,fiction and serials that \"Captain\" pays tribute to were entirely different. Especially so for those who experienced those entertainments when they were children. This film is almost completely devoid of the charm and magnetic attraction of those. Of course we know the leads will get into and out of scrapes but there has to be some tension and drama. Toward the climax of \"Captain\" Law and Paltrow have ten minutes to prevent catastrophe and by the time they get down to five minutes they are walking not running toward their goal. They take time out for long looks and unnecessary conversation and the contemplation of a fallen foe with 30 seconds left to tragedy. Of course one expects certain conventions to be included but a good director would have kept up some sense of urgency.<br /><br />One doesn\\'t expect films like this to necessarily \"make sense\". One does expect them to be fun, thrilling and to have some sense of interior logic. \"Captain\" has almost none. Remember when Law and Paltrow are being pursued by the winged creatures and they reach a huge chasm which they cross via a log bridge? Well how come they are perfectly safe from those creatures when they reach the other side? They can FLY!!! The chasm itself means nothing to them. The bridge is unnecessary for them so where is the escape? If the land across the chasm is \\'forbidden\\' to the flying creatures the film made no effort to let us know how or why or even if.<br /><br />I know that Paltrow and Law (both of whom have given fine performances in the past) were playing \"types\" but both were pretty flat. Only Giovanni Ribisi (who showed himself capable of great nuance here) and Angelina Jolie seemed to give any \"oomph\" to their roles although Omid Djalili seemed like he could have handled a little more if he\\'d only been given the chance. He did a pretty good job anyway considering how he was basically wasted.<br /><br />The film had a great \\'look\\' but there are so many ways in which CGI distracts. CGI works best when it is used for the fantastical, when it is used to create creatures who don\\'t exist in nature or for scientific or magical spectacular. When it is used to substitute for natural locations it disappoints. There is no real sense of wonder. A CGI mountain doesn\\'t have any of the stateliness or sense of awe and foreboding that a real mountain does. I know that the design of this film was quite deliberate and it wasn\\'t necessarily supposed to LOOK real but shouldn\\'t it FEEL that way? It just didn\\'t. <br /><br />As for the weak and clichéd script...homage is no excuse. Even so, had the movie had some thrills and dramatic tension it might still have been enjoyable. \"The Last Samurai\" was as predictable as the days of the week and I am no fan of Tom Cruise but it had everything that \"Captain\" didn\\'t most notably it drew the viewer into its world and made us accept its rules and way of being in a way that \"Sky Captain\" most definitely did not.<br /><br />I\\'d like to see a similar approach taken for films about comic book heroes of the 30\\'s and 40\\'s. The original (Jay Garrick) Flash or Green Lantern (Alan Scott) come to mind as being ripe for such treatment. Maybe the better, more well known and fully realized characters that those character are would make for a much better film. It would be hard to be worse.',\n",
              " \"The humor in Who's Your Daddy is such poor taste that I actually closed my eyes in certain scenes. Close ups of semen are not funny! Nobody thinks they are. People get nervous when they see something so gross and to hide their nervousness, they laugh. Watching Who's Your Daddy gave me a disgusting nervous feeling.\",\n",
              " 'Johnny and Jeremy are vampires of sorts. Minus the fangs, of course. They\\'re dark, bitter creatures with nothing better to do than to spread their own misery. Through their charms (namely a sharp tongue and a fat wallet, respectively) they seduce desperate souls, who they proceed to torment and victimize. That\\'s more or less the basis of this black comedy, as I understand it.<br /><br />It\\'s not a blend of black humor that I can easily subscribe to, partly because it bothers me to imagine the audience rooting for the sleazy, main character. I did enjoy, however, the sound and the melody of the rapid-fire (and supposedly very witty) remarks. I was very impressed by the cast\\'s strong acting, particularly David Thelis\\'s; only the character of Jeremy seemed too bi-dimensional. The photography and the music, both dramatic and somber, work very well together. <br /><br />What really turns me off about \"Naked\" (and the main reason I\\'d never recommend it to anyone) is the way it repeatedly seems to present misogyny as a valid way to vent one\\'s angst. In other words, in a world that sucks so bad, what difference does it make if one inflicts some pain on girls, right? To suggest (as some have on this website) that Johnny is not so unkind a person because he\\'s not as rough on girls as Jeremy, seems completely absurd to me. They\\'re both terrible, nasty people. And they\\'re particularly keen on hurting women every single time they get a chance. One could argue that Johnny eventually gets what he deserves, as if his bad karma suddenly swung straight back and bit him in the ass. But still, his and Jeremy\\'s sadistic behavior are treated to a certain degree as a laughing matter. And I could be wrong, but I\\'m guessing that most people who absolutely love this movie also find that aspect of the film darkly comical.',\n",
              " \"Live! Yes, but not kicking.<br /><br />True story: Some time ago, a Dutch TV station made an announcement that they were going to air a new reality show. A contest rather. The main participant in this show would be a woman who was dying of something terrible and she would be donating her kidneys to one lucky person with progressive kidney failure. For real.<br /><br />The country and the international media were all over this story like flies on a turd, saying it was appalling, immoral, what-is-this-world-coming-to, and the like. In a way, I had to agree.<br /><br />As the months passed, the tension built up to a degree that the government was mostly occupied by the issue of whether they should let this show go ahead or not, instead of running the country.<br /><br />The show did air and right up to the last moment they were pushing ahead. And up to the last moment the country was up in arms, the Prime Minister making speeches, every newspaper writing about it, everyone in the country holding their breaths. And the network pushed on. Towards a new frontier in television. And they definitely succeeded in doing just that. They pushed the envelope.<br /><br />The show aired and we all watched a terminally ill woman selecting the right candidate to receive her kidneys so he or she would live, whilst she would die shortly after.<br /><br />And then, in the last moments of the show it was revealed that it was a partial hoax. The woman was not ill, but all the candidates were. There was no kidney auction. The whole show, that, with the publicity and the commercials and all the discussions, built up for months to a fantastic climax, was a publicity stunt to focus attention on the problem of major shortages in organ donors. The man who founded this particular network himself died of kidney disease.<br /><br />Now THIS is television. Leaving everybody far behind in amazement.<br /><br />Don't give me a poorly acted, poorly directed flick about some woman trying to get a Russian Roulette show on American TV.<br /><br />As if.<br /><br />*Spoiler* As if I'm going to believe they would get this through the FCC. As if I'm going to believe this would get through the US Supreme Court on the basis of free expression. As if I'm gonna believe the ridiculous ending where this woman pulled it off and has conscience issues because some guy shot himself on air.<br /><br />It's all been done before. Watch Running Man with Arnold instead. At least it had a semi good ending.<br /><br />*Spoiler* This is an appallingly bad piece of film, together with a ridiculous ending. So she gets shot in the end, is that supposed to make us movie going public feel better after we leave the theater because there was some kind of justice? Don't take my word for it, but I would say this: leave this one alone and watch a test pattern instead, you'll get more quality.\",\n",
              " \"Somewhere inside this movie is a half-hour episode from The Twilight Zone trying to get out. Whereas Cube was taut, well-made, claustrophobic and mind-engaging, I'm afraid Cypher is a bloated, tedious rehash of several well-worn themes which just don't add up to much, especially if you have seen almost any other half-way decent sci-fi film before.<br /><br />Cypher manages to drag all the way through its relatively short 95 minutes right to the incompetent ending. None of the characters spark off each other, and for a film made in 2002 the technology is truly cheesy. It is difficult to connect this tired and uninspired movie with the director of Cube. It's not a bad movie, but it is most definitely not a good one.<br /><br />When you've watched the grass grow and paint dry and are bored of your stick insects then by all means watch this film, but the other activities will probably prove more stimulating.\",\n",
              " 'It was almost worth sitting through this entire god-awful \"film\" just to know that I can never experience anything as bad as this again. Acting - 0, script - 0, fight scenes - 0, male lead - 0 (cheddar bob from eight mile as a suave war hero who gets the girl), Nadia Bjorlin - 10 (She is gorgeous and not a terrible actress). This is the criteria I used to average it out to a two. I lost count but I believe ever movie cliché, ever, is in this movie. When the driver that supposedly killed her father miraculously shows up at the end to race against her, from out of nowhere it cemented the previous statement. Plus he just shows up for no reason. He was never even mentioned before. I don\\'t know what else to say here. Just watch it when it comes out on TV in a couple years. At least that way some of it will be edited out for commercials.',\n",
              " 'Disappearance is about a couple who take their family on vacation in New Mexico and find themselves in deep trouble after taking a detour off the main highway to visit a town that was seemingly abandoned in 1948 for unknown reasons. The town of Weaver seems harmless at first and has tourist appeal until the family is stranded there overnight and they begin to have good reason to suspect that others have experienced their same predicament with fatal outcomes. The Henleys watch a Blair-Witch-Project-esquire video diary left by the town\\'s last victim, which ironically demonstrates the best performance of anyone in this movie. Although Hamlin and Dey\\'s performances are much better than the supporting casts\\', their emotional affect seems \"flat\" to me throughout the movie. <br /><br />Disappearance has appeal for most of the movie as there is much suspense and good direction. However, the plot takes unexpected and implausible turns that seemingly make no sense. Worse yet it that there really is no understanding of what exactly is going on in the movie, which makes the bizarre ending less tolerable. It appeared to me that the movie makers were so focused on making a stream of suspenseful scenes, that they threw away all the elements of good story making: plot development, gradual explanation of themes and symbols that lead to a cohesive solution/outcome. <br /><br />The most difficult aspect of the movie for me was that the first three-quarter of it was spent building up tension and curiosity about certain aspects of the plot that were then suddenly disposed of as if we didn\\'t deserve an explanation: <br /><br />What was the significance of the Indian symbols on the walls? What happened to the original people of Weaver? What was the connection with the people at the dinner? What did the Sheriff know? What did the missing boy discover if anything?<br /><br />This was, I believe, a bad move, since it engendered some resentment. I had invested quite a bit of brainpower into hypothesizing some plausible explanations for some of these plot turns and strange events, only to have the movie makers simply end it without giving an answer to any of these things. These are some nice cliffhangers for the ending of a miniseries that is about to pickup again next week, but a totally frustrating and inappropriate ending for a stand-alone movie.',\n",
              " 'The world is facing imminent destruction and a suicide mission is sent to the Sun to avert catastrophe by firing a bomb into its fiery heart: yes, it\\'s Solar Crisis, aka Crisis 2050, which burned up a huge chunk of change that\\'s never apparent on screen back in 1990 and returned barely enough to buy a Happy Meal for each of the cast in Japan before going straight to video (remember them?) in a re-edited version credited to one Alan Smithee. The plot hook\\'s pretty much the same as Sunshine - suicide mission to the Sun, saboteur on board, logic cast adrift - except that this time they\\'re not trying to reignite the sun but to prematurely detonate a solar flare before it can reach Earth. With a talking bomb. Voiced by Paul Williams. Who wants to be promoted so the crew will take him more seriously\\x85 Given that the cast also includes Jack Palance at his most dementedly OTT, Charlton Heston at his most rigid, top-liner Tim Matheson at his most anonymous, the original Hills Have Eyes\\' unforgettable Michael Berryman (you may not remember the name, but you DO remember that face) and Peter Boyle as the industrialist out to sabotage the mission because, er, if it succeeds the world will be saved but his share price will go down, you\\'d expect if not a laugh-a-minute at least a laugh every reel. No joy. This is the worst kind of bad movie: a boring one. The fate of the world may be hanging in the balance but the whole film is shot with a complete lack of urgency or momentum at the same unvarying deadly slow pace. There\\'s low-key and there\\'s walking through it, but here the cast don\\'t even do that. Instead, they just stand still looking at screens in near darkness for most of the time. You keep on hoping for Paul Williams\\' talking bomb to suffer an existential crisis, but instead the film just... stands there, doing next to nothing. Literally. This is one of the most inert movies ever made \\x96 so inert that if Clive Owen had been cast, he\\'d almost have looked lively by comparison. Even a poorly explained suicidal repair attempt fails to raise a fritter of interest since it mostly involves, yep, the cast just standing still looking at screens in near darkness. Even when the bomb prematurely goes into countdown before being launched they deal with the new crisis by\\x85 standing still looking at screens in near darkness as if they had all the time in the world. Merchant-Ivory films have better action scenes.<br /><br />Things aren\\'t much livelier down on Earth where the movie spends most of it\\'s running time with Matheson\\'s son/Chuck\\'s grandson Corin Nemec trying to hitch a ride to the spaceport across an arid landscape with Palance\\'s insane desert artist \"looking for that note out there while the chicks still dig me\" while waylaid by rejects from a Mad Max ripoff and evil corporate suits who track him down so they can\\x85 release him on a nice beach. Just don\\'t expect logic, if you haven\\'t already guessed that much. Best moment? A ditzy girl in a bar describing Jack Palance as \"An old guy with white hair and a face like rotting leather,\" though Chucky Baby taking out the villain\\'s aircraft with a bazooka fired from the hip from an office window or beating up a barfly who likes his beret are welcome morsels of camp in a film that for 99% of it\\'s running time offers a whole lot of nuttin\\'. Richard C. Sarafian\\'s slightly longer original cut that played in Japan offers an additional six minutes but cries out to be cut down to a more manageable 17 minutes: the director of Vanishing Point must have thanked his lucky stars when the re-edit gave him an excuse to take his name off the film. A film so bad it\\'s not good, and painfully unfunny with it',\n",
              " 'Richard Farnsworth is one of my favorite actors. He usually gives solid performances, such as in The Straight Story, and The Grey Fox. He also does fairly well here, but the rest of the film suffers from a low budget, poor writing, and so-so photography. The Miller-Movie formula gives it a 4. Richard gets a 5.']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4-LSwiEsf5a",
        "outputId": "1e285637-e9bf-407a-bce9-63ac6fb8f494"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels[-10:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PylnL2C8s4-P",
        "outputId": "313bb5b2-c4e2-49c2-efaa-fe788c7adf15"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "maxlen = 100   # skracamy recenzje do 100 słów\n",
        "num_words = 10000    # 10000 najczęściej pojawiających się słów\n",
        "embedding_dim = 100\n",
        "\n",
        "tokenizer = Tokenizer(num_words=num_words)\n",
        "tokenizer.fit_on_texts(train_texts)"
      ],
      "metadata": {
        "id": "Xm8gcRoPs9W6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(tokenizer.index_word.items())[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZwbzwcZuJqE",
        "outputId": "e358c7f4-ca27-426a-dfd8-743da533de27"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 'the'),\n",
              " (2, 'and'),\n",
              " (3, 'a'),\n",
              " (4, 'of'),\n",
              " (5, 'to'),\n",
              " (6, 'is'),\n",
              " (7, 'br'),\n",
              " (8, 'in'),\n",
              " (9, 'it'),\n",
              " (10, 'i'),\n",
              " (11, 'this'),\n",
              " (12, 'that'),\n",
              " (13, 'was'),\n",
              " (14, 'as'),\n",
              " (15, 'for'),\n",
              " (16, 'with'),\n",
              " (17, 'movie'),\n",
              " (18, 'but'),\n",
              " (19, 'film'),\n",
              " (20, 'on')]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "print(sequences[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMRJaJZIuQou",
        "outputId": "d2144ec4-b8d5-44b7-e986-05442e3a6b41"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[5340, 2103, 242, 233, 288, 2, 13, 3, 52, 3930, 434, 13, 28, 4, 24, 1123, 9259, 26, 7693, 3, 4287, 9092, 4, 105, 2, 35, 26, 13, 2718, 5, 3, 367, 30, 219, 28, 55, 8, 24, 110, 18, 1, 105, 26, 314, 9457, 852, 170, 4138, 8, 2706, 4, 8422, 236, 13, 52, 8, 1, 2813, 4, 1, 102, 847, 3070, 13, 3, 4588, 1583, 57, 44, 5340, 2128, 3, 8288, 236, 2615, 3815, 13, 157, 384, 177, 2, 2753, 9458, 6, 14, 14, 91, 8289, 91, 695, 982, 6, 6792, 8, 3, 6436, 93, 7, 7, 2, 187, 1, 1853, 323, 384, 5674, 3, 3559, 209, 3, 375, 1425, 31, 2, 34, 5514, 65, 365, 6, 8, 366, 724, 8, 3, 3022, 5, 294, 16, 65, 5515, 33, 3931, 5, 170, 1166, 16, 254, 82, 2, 180, 89, 137, 14, 4252, 85, 1, 4, 65, 149, 862, 95, 1, 104, 1838, 377, 5, 41, 1, 4, 65, 375, 7, 7, 8, 2706, 4, 5456, 1381, 2, 1, 2425, 815, 26, 66, 30, 24, 937, 188, 1918, 5, 199, 110, 5, 11, 497, 16, 2070, 3606, 1, 111, 1157, 3, 70, 4716, 6437, 16, 102, 34, 25, 3366, 70, 570, 8423, 2, 5171, 12, 23, 41, 34, 33, 63, 23, 6887, 39, 901, 695, 299, 396, 804, 1032, 3, 75, 799, 7799, 3, 114, 50, 1, 19, 16, 11, 75, 6438, 36, 5, 6192, 602, 108, 841, 57, 1, 6793, 4801, 8, 1, 5558, 6, 33, 303, 5, 27, 1095, 2, 5, 65, 71, 5, 409, 95, 258, 1, 308, 6, 512, 1056, 8, 3, 930, 7, 7, 42, 5, 132, 9, 5340, 105, 112, 1448, 53, 5, 24, 2990, 14, 2753, 9458, 3309, 2410], [1740, 1702, 200, 27, 1189, 32, 3475, 5, 695, 1148, 3975, 2471, 2, 17, 6985, 18, 9, 1359, 114, 4, 1, 1230, 4, 46, 4, 1, 115, 36, 145, 3866, 28, 555, 12, 3108, 4, 1, 19, 5794, 20, 723, 39, 21, 28, 1, 105, 8169, 10, 89, 101, 11, 6, 30, 29, 280, 2058, 1395, 4, 1, 105, 1213, 2, 6985, 12, 1702, 4077, 3331, 5, 68, 1092, 272, 258, 35, 15, 145, 34, 2567, 145, 51, 33, 68, 473, 11, 19, 6, 217, 337, 4139, 4, 1, 1379, 2, 3210, 4, 145, 4, 261, 72, 121, 1, 829, 77, 76, 80, 2, 43, 4, 18, 47, 45, 5, 27, 46, 1071, 2, 450, 1839, 1, 1326, 4, 1702, 1160, 2, 4802, 25, 744, 231, 5, 3607, 9654, 2, 31, 1, 55, 33, 76, 177, 5, 674, 231, 33, 23, 1282, 21, 617, 1839, 65, 3348, 33, 190, 55, 43, 15, 193, 269, 2, 1741, 2588, 2, 1, 4, 3, 2903, 16, 1085, 1570, 314, 5, 1515, 4, 261, 28, 6354, 810, 5516, 5, 27, 1919, 18, 3, 49, 164, 59, 25, 825, 53, 46, 278, 4, 8926, 7, 7, 28, 149, 532, 105, 37, 11, 5, 2696, 94, 278, 28, 124, 532, 95, 5, 27, 250, 3008, 2, 5, 25, 46, 278, 4, 7476, 2086, 1702, 45, 217, 597, 374, 51, 1160, 2, 4802, 23, 109, 6986, 31, 1, 2365, 2, 33, 2095, 3, 663, 60, 33, 1656, 2847, 3, 3023, 70, 86, 213, 33, 23, 946, 2270, 36, 145, 2365, 51, 33, 2095, 1, 82, 496, 33, 67, 2218, 1, 407, 814, 161, 5, 95, 1, 3023, 6, 1741, 15, 95, 35, 118, 6, 1, 1086, 44, 1, 1537, 635, 1, 6, 5, 1, 1544, 2365, 1, 19, 90, 54, 778, 5, 384, 175, 121, 86, 39, 135, 39, 57, 44, 7, 7, 10, 121, 12, 4802, 2, 1160, 196, 4, 931, 25, 345, 475, 351, 8, 1, 498, 68, 392, 2104, 18, 196, 68, 181, 1032, 61, 34, 1174, 306, 2246, 4, 84, 9459, 130, 2, 7364, 7093, 465, 5, 199, 98, 5, 65, 552, 259, 465, 37, 26, 97, 25, 2387, 3, 114, 50, 44, 2697, 61, 74, 345, 1, 576, 26, 119, 3, 181, 49, 289, 550, 1066, 86, 26, 13, 688, 1050, 7, 7, 1, 19, 66, 3, 84, 18, 47, 23, 35, 108, 768, 8, 60, 1679, 1679, 492, 115, 51, 9, 6, 340, 15, 1, 51, 9, 6, 340, 5, 984, 2365, 34, 89, 1773, 8, 873, 39, 15, 3734, 39, 2537, 2087, 51, 9, 6, 340, 5, 7576, 15, 1245, 1975, 9, 8595, 47, 6, 54, 144, 278, 4, 591, 3, 1679, 2521, 149, 25, 98, 4, 1, 39, 278, 4, 4288, 2, 8596, 12, 3, 144, 2521, 124, 10, 121, 12, 1, 1588, 4, 11, 19, 13, 176, 6439, 2, 9, 283, 2696, 420, 5, 165, 144, 18, 1609, 9, 232, 12, 93, 9, 40, 158, 7, 7, 14, 15, 1, 812, 2, 2707, 226, 3475, 6, 54, 1335, 57, 35, 66, 1, 17, 66, 46, 3476, 2, 902, 1071, 9, 235, 128, 25, 74, 734, 1, 233, 3626, 13, 14, 725, 14, 1, 501, 4, 1, 1264, 2, 10, 241, 54, 334, 4, 824, 3765, 18, 9, 66, 282, 12, 1702, 158, 88, 3712, 9, 2262, 1, 526, 80, 91, 179, 2, 90, 175, 1774, 91, 2263, 2, 93, 4, 109, 8, 3, 93, 12, 1740, 1702, 88, 404, 119, 21, 7, 7, 471, 37, 5, 64, 3, 726, 1478, 620, 15, 105, 41, 695, 271, 1730, 4, 1, 5621, 2, 7180, 1, 201, 3050, 3169, 39, 1414, 1605, 1088, 213, 5, 327, 14, 109, 15, 138, 2196, 276, 1, 125, 50, 70, 570, 2, 1311, 1703, 102, 12, 145, 106, 23, 59, 94, 15, 3, 73, 125, 19, 9, 59, 27, 251, 5, 27, 430], [1, 482, 8, 868, 126, 4416, 6, 138, 335, 1293, 12, 10, 162, 4558, 58, 520, 8, 810, 136, 488, 1965, 4, 23, 21, 160, 1334, 1287, 33, 23, 81, 76, 4417, 51, 33, 64, 139, 35, 2735, 2, 5, 2411, 65, 33, 459, 146, 868, 126, 4416, 516, 69, 3, 2306, 4417, 544]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sprawdźmy jak to działa..."
      ],
      "metadata": {
        "id": "DirxWKt4zxLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "print('Sequence 0: ' , sequences[0])\n",
        "print('Długość sequence[0] = ' , len(sequences[0]))\n",
        "print('Pierwsze zdanie z sequence: ' ,train_texts[0])\n",
        "print('Ilość znaków w zdaniu pierwszym = ' ,len(train_texts[0]))\n",
        "\n",
        "print('Pierwszy znak w train_texts = ' ,train_texts[0][0])\n",
        "print('Pierwszy wyraz w zdaniu , index kolejności występowania = ' ,sequences[0][0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf53a964-7f97-4c4c-f26d-cccb35d338e5",
        "id": "y7pfGmlUu8gJ"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence 0:  [5340, 2103, 242, 233, 288, 2, 13, 3, 52, 3930, 434, 13, 28, 4, 24, 1123, 9259, 26, 7693, 3, 4287, 9092, 4, 105, 2, 35, 26, 13, 2718, 5, 3, 367, 30, 219, 28, 55, 8, 24, 110, 18, 1, 105, 26, 314, 9457, 852, 170, 4138, 8, 2706, 4, 8422, 236, 13, 52, 8, 1, 2813, 4, 1, 102, 847, 3070, 13, 3, 4588, 1583, 57, 44, 5340, 2128, 3, 8288, 236, 2615, 3815, 13, 157, 384, 177, 2, 2753, 9458, 6, 14, 14, 91, 8289, 91, 695, 982, 6, 6792, 8, 3, 6436, 93, 7, 7, 2, 187, 1, 1853, 323, 384, 5674, 3, 3559, 209, 3, 375, 1425, 31, 2, 34, 5514, 65, 365, 6, 8, 366, 724, 8, 3, 3022, 5, 294, 16, 65, 5515, 33, 3931, 5, 170, 1166, 16, 254, 82, 2, 180, 89, 137, 14, 4252, 85, 1, 4, 65, 149, 862, 95, 1, 104, 1838, 377, 5, 41, 1, 4, 65, 375, 7, 7, 8, 2706, 4, 5456, 1381, 2, 1, 2425, 815, 26, 66, 30, 24, 937, 188, 1918, 5, 199, 110, 5, 11, 497, 16, 2070, 3606, 1, 111, 1157, 3, 70, 4716, 6437, 16, 102, 34, 25, 3366, 70, 570, 8423, 2, 5171, 12, 23, 41, 34, 33, 63, 23, 6887, 39, 901, 695, 299, 396, 804, 1032, 3, 75, 799, 7799, 3, 114, 50, 1, 19, 16, 11, 75, 6438, 36, 5, 6192, 602, 108, 841, 57, 1, 6793, 4801, 8, 1, 5558, 6, 33, 303, 5, 27, 1095, 2, 5, 65, 71, 5, 409, 95, 258, 1, 308, 6, 512, 1056, 8, 3, 930, 7, 7, 42, 5, 132, 9, 5340, 105, 112, 1448, 53, 5, 24, 2990, 14, 2753, 9458, 3309, 2410]\n",
            "Długość sequence[0] =  287\n",
            "Pierwsze zdanie z sequence:  Bernard Rapp passed away last year and was a very cultured journalist. Cinema was one of his biggest passions (he penned a vast worldwide dictionary of films) and so he was bound to wield a camera at least one time in his life. But the films he left garnered lukewarm reviews: \"Tiré à Part\" (1996) in spite of Terence Stamp's sensational performance was very caricatured in the depiction of the characters, \"une Affaire De Goût\" (2000) was a slick affair even if Bernard Giraudeau delivered a perverse performance, \"Pas Si Grave\" (2003) was another let-down and \"un Petit Jeu Sans Conséquence\" is as underwhelming as its predecessors. Its comic potential is exploited in a flimsy way.<br /><br />And however, the starting idea let predict a twirling, spiritual comedy. A couple held by Yvan Attal and Sandrine Kiberlain who invited their friends is in full moving in a lascivious mansion. To play with their guests, they pretend to part company with each other. And things don't go as planned because the announcement of their separation doesn't surprise them. The two lovers start to ponder about the validity of their couple.<br /><br />In spite of lush scenery and the promising material he had at his disposal, Rapp's undistinguished directing can't manage to give life to this game with unexpected consequences. The plot follows a well-worn pattern with characters who have specific well-known functions and masks that are unveiled about who they really are. Verbal or situation comic effects often fall flat. A bad editing fades a little more the film with this bad habit from Rapp to abruptly cut many sequences. Even the actors' sincere input in the venture is debatable. They seem to be bored and to recite their texts than to live them, especially Sandrine Kiberlain. The audience is soon caught in a deep torpor.<br /><br />It's regrettable to say it: Bernard Rapp's films never lived up to his intentions as \"un Petit Jeu sans Conséquence\" bears witness.\n",
            "Ilość znaków w zdaniu pierwszym =  1966\n",
            "Pierwszy znak w train_texts =  B\n",
            "Pierwszy wyraz w zdaniu , index kolejności występowania =  5340\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_words_from_indexes(indexArr, tokenizer):\n",
        "    # Pobranie słów odpowiadających indeksom z indexArr\n",
        "    words = [tokenizer.index_word.get(index, None) for index in indexArr]\n",
        "    return words\n",
        "\n",
        "indexArr = [5340, 2103, 242, 233, 288, 2, 13, 3, 52, 3930, 434, 13]\n",
        "print(get_words_from_indexes(indexArr, tokenizer))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NT2iymaXw-WI",
        "outputId": "77e11f03-1b9d-42d0-c955-485116037099"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['bernard', 'passed', 'away', 'last', 'year', 'and', 'was', 'a', 'very', 'journalist', 'cinema', 'was']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "wracamy do tutoriala"
      ],
      "metadata": {
        "id": "0FxwVOkpzsHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = tokenizer.word_index\n",
        "print(f'{len(word_index)} unikatowych słów.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63KM0Jx5zB1e",
        "outputId": "9d51fc2e-31e0-4dcc-fe75-d6ed37ef2965"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "88582 unikatowych słów.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# skracamy recenzje do pierwszych 100 słów\n",
        "train_data = pad_sequences(sequences, maxlen=maxlen)\n",
        "train_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFwbs60dzuok",
        "outputId": "84f9b120-210a-4bc6-9461-dfc48fa82392"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MacSi4q0FY4",
        "outputId": "71be9614-7fac-44ba-98ab-4020ce683c36"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3606,    1,  111, 1157,    3,   70, 4716, 6437,   16,  102,   34,\n",
              "          25, 3366,   70,  570, 8423,    2, 5171,   12,   23,   41,   34,\n",
              "          33,   63,   23, 6887,   39,  901,  695,  299,  396,  804, 1032,\n",
              "           3,   75,  799, 7799,    3,  114,   50,    1,   19,   16,   11,\n",
              "          75, 6438,   36,    5, 6192,  602,  108,  841,   57,    1, 6793,\n",
              "        4801,    8,    1, 5558,    6,   33,  303,    5,   27, 1095,    2,\n",
              "           5,   65,   71,    5,  409,   95,  258,    1,  308,    6,  512,\n",
              "        1056,    8,    3,  930,    7,    7,   42,    5,  132,    9, 5340,\n",
              "         105,  112, 1448,   53,    5,   24, 2990,   14, 2753, 9458, 3309,\n",
              "        2410],\n",
              "       [   9,   66,  282,   12, 1702,  158,   88, 3712,    9, 2262,    1,\n",
              "         526,   80,   91,  179,    2,   90,  175, 1774,   91, 2263,    2,\n",
              "          93,    4,  109,    8,    3,   93,   12, 1740, 1702,   88,  404,\n",
              "         119,   21,    7,    7,  471,   37,    5,   64,    3,  726, 1478,\n",
              "         620,   15,  105,   41,  695,  271, 1730,    4,    1, 5621,    2,\n",
              "        7180,    1,  201, 3050, 3169,   39, 1414, 1605, 1088,  213,    5,\n",
              "         327,   14,  109,   15,  138, 2196,  276,    1,  125,   50,   70,\n",
              "         570,    2, 1311, 1703,  102,   12,  145,  106,   23,   59,   94,\n",
              "          15,    3,   73,  125,   19,    9,   59,   27,  251,    5,   27,\n",
              "         430],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    1,  482,    8,  868,  126, 4416,    6,  138,  335,\n",
              "        1293,   12,   10,  162, 4558,   58,  520,    8,  810,  136,  488,\n",
              "        1965,    4,   23,   21,  160, 1334, 1287,   33,   23,   81,   76,\n",
              "        4417,   51,   33,   64,  139,   35, 2735,    2,    5, 2411,   65,\n",
              "          33,  459,  146,  868,  126, 4416,  516,   69,    3, 2306, 4417,\n",
              "         544]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# chcę zobaczyć o co tu chodzi..\n",
        "get_words_from_indexes(train_data[:3][0], tokenizer)\n",
        "\n",
        "# wygląda że wypisuje słowa w kol. występowania"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utdi7DcZ0JN8",
        "outputId": "3fc6229e-7fbc-4c90-b811-11294718b6be"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['consequences',\n",
              " 'the',\n",
              " 'plot',\n",
              " 'follows',\n",
              " 'a',\n",
              " 'well',\n",
              " 'worn',\n",
              " 'pattern',\n",
              " 'with',\n",
              " 'characters',\n",
              " 'who',\n",
              " 'have',\n",
              " 'specific',\n",
              " 'well',\n",
              " 'known',\n",
              " 'functions',\n",
              " 'and',\n",
              " 'masks',\n",
              " 'that',\n",
              " 'are',\n",
              " 'about',\n",
              " 'who',\n",
              " 'they',\n",
              " 'really',\n",
              " 'are',\n",
              " 'verbal',\n",
              " 'or',\n",
              " 'situation',\n",
              " 'comic',\n",
              " 'effects',\n",
              " 'often',\n",
              " 'fall',\n",
              " 'flat',\n",
              " 'a',\n",
              " 'bad',\n",
              " 'editing',\n",
              " 'fades',\n",
              " 'a',\n",
              " 'little',\n",
              " 'more',\n",
              " 'the',\n",
              " 'film',\n",
              " 'with',\n",
              " 'this',\n",
              " 'bad',\n",
              " 'habit',\n",
              " 'from',\n",
              " 'to',\n",
              " 'abruptly',\n",
              " 'cut',\n",
              " 'many',\n",
              " 'sequences',\n",
              " 'even',\n",
              " 'the',\n",
              " \"actors'\",\n",
              " 'sincere',\n",
              " 'in',\n",
              " 'the',\n",
              " 'venture',\n",
              " 'is',\n",
              " 'they',\n",
              " 'seem',\n",
              " 'to',\n",
              " 'be',\n",
              " 'bored',\n",
              " 'and',\n",
              " 'to',\n",
              " 'their',\n",
              " 'than',\n",
              " 'to',\n",
              " 'live',\n",
              " 'them',\n",
              " 'especially',\n",
              " 'the',\n",
              " 'audience',\n",
              " 'is',\n",
              " 'soon',\n",
              " 'caught',\n",
              " 'in',\n",
              " 'a',\n",
              " 'deep',\n",
              " 'br',\n",
              " 'br',\n",
              " \"it's\",\n",
              " 'to',\n",
              " 'say',\n",
              " 'it',\n",
              " 'bernard',\n",
              " 'films',\n",
              " 'never',\n",
              " 'lived',\n",
              " 'up',\n",
              " 'to',\n",
              " 'his',\n",
              " 'intentions',\n",
              " 'as',\n",
              " 'un',\n",
              " 'sans',\n",
              " 'bears',\n",
              " 'witness']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = np.asarray(train_labels)\n",
        "train_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cT9N-uN00fhS",
        "outputId": "c0974e86-e24a-419c-b3e1-59ef01438a93"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# przemieszanie próbek\n",
        "indices = np.arange(train_data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "train_data = train_data[indices]\n",
        "train_labels = train_labels[indices]\n",
        "\n",
        "train_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJ4bVyKL09rX",
        "outputId": "849a407a-d3bc-4a3e-f4e6-2a7332d157a9"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# podział na zbiór treningowy i walidacyjny\n",
        "training_samples = 15000\n",
        "validation_samples = 10000\n",
        "\n",
        "X_train = train_data[:training_samples]\n",
        "y_train = train_labels[:training_samples]\n",
        "X_val = train_data[training_samples: training_samples + validation_samples]\n",
        "y_val = train_labels[training_samples: training_samples + validation_samples]"
      ],
      "metadata": {
        "id": "uDWm8v4l1AS8"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# budowa modelu\n",
        "# Embedding(input_dim, output_dim)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(num_words, embedding_dim, input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "gE83FZ-31E4P",
        "outputId": "c5e980ca-7b72-4008-9f95-e294f36115e5"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "BbJakRQe1JzP"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train,\n",
        "                    y_train,\n",
        "                    batch_size=32,\n",
        "                    epochs=5,\n",
        "                    validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZeSlbSQ1Ncx",
        "outputId": "19898019-57fa-4cac-e0f0-b75b81e51744"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 21ms/step - accuracy: 0.6601 - loss: 0.5929 - val_accuracy: 0.8242 - val_loss: 0.3847\n",
            "Epoch 2/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - accuracy: 0.9265 - loss: 0.2093 - val_accuracy: 0.8310 - val_loss: 0.3964\n",
            "Epoch 3/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 18ms/step - accuracy: 0.9917 - loss: 0.0387 - val_accuracy: 0.8237 - val_loss: 0.5185\n",
            "Epoch 4/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - accuracy: 0.9991 - loss: 0.0051 - val_accuracy: 0.8245 - val_loss: 0.6278\n",
            "Epoch 5/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 5.1373e-04 - val_accuracy: 0.8240 - val_loss: 0.7095\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_hist(history):\n",
        "    import pandas as pd\n",
        "    import plotly.graph_objects as go\n",
        "    hist = pd.DataFrame(history.history)\n",
        "    hist['epoch'] = history.epoch\n",
        "\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Scatter(x=hist['epoch'], y=hist['accuracy'], name='accuracy', mode='markers+lines'))\n",
        "    fig.add_trace(go.Scatter(x=hist['epoch'], y=hist['val_accuracy'], name='val_accuracy', mode='markers+lines'))\n",
        "    fig.update_layout(width=1000, height=500, title='accuracy vs. val accuracy', xaxis_title='Epoki', yaxis_title='accuracy', yaxis_type='log')\n",
        "    fig.show()\n",
        "\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Scatter(x=hist['epoch'], y=hist['loss'], name='loss', mode='markers+lines'))\n",
        "    fig.add_trace(go.Scatter(x=hist['epoch'], y=hist['val_loss'], name='val_loss', mode='markers+lines'))\n",
        "    fig.update_layout(width=1000, height=500, title='loss vs. val loss', xaxis_title='Epoki', yaxis_title='loss', yaxis_type='log')\n",
        "    fig.show()\n",
        "\n",
        "plot_hist(history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RJT1ZeJG1Stm",
        "outputId": "4e287230-78af-424d-ebdf-c6f726a7b9a7"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"43689247-7a5b-4ead-adb9-351d32d4d9d8\" class=\"plotly-graph-div\" style=\"height:500px; width:1000px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"43689247-7a5b-4ead-adb9-351d32d4d9d8\")) {                    Plotly.newPlot(                        \"43689247-7a5b-4ead-adb9-351d32d4d9d8\",                        [{\"mode\":\"markers+lines\",\"name\":\"accuracy\",\"x\":[0,1,2,3,4],\"y\":[0.7487333416938782,0.9276000261306763,0.9922666549682617,0.9991999864578247,0.9999333620071411],\"type\":\"scatter\"},{\"mode\":\"markers+lines\",\"name\":\"val_accuracy\",\"x\":[0,1,2,3,4],\"y\":[0.8241999745368958,0.8309999704360962,0.8237000107765198,0.8245000243186951,0.8240000009536743],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"yaxis\":{\"title\":{\"text\":\"accuracy\"},\"type\":\"log\"},\"width\":1000,\"height\":500,\"title\":{\"text\":\"accuracy vs. val accuracy\"},\"xaxis\":{\"title\":{\"text\":\"Epoki\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('43689247-7a5b-4ead-adb9-351d32d4d9d8');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"1c630b2f-00e8-4073-9e25-8d201a9a27db\" class=\"plotly-graph-div\" style=\"height:500px; width:1000px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"1c630b2f-00e8-4073-9e25-8d201a9a27db\")) {                    Plotly.newPlot(                        \"1c630b2f-00e8-4073-9e25-8d201a9a27db\",                        [{\"mode\":\"markers+lines\",\"name\":\"loss\",\"x\":[0,1,2,3,4],\"y\":[0.49465757608413696,0.19788159430027008,0.034562934190034866,0.0043626222759485245,0.00044537094072438776],\"type\":\"scatter\"},{\"mode\":\"markers+lines\",\"name\":\"val_loss\",\"x\":[0,1,2,3,4],\"y\":[0.3847225308418274,0.3963990807533264,0.5185076594352722,0.6277966499328613,0.7094594240188599],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"yaxis\":{\"title\":{\"text\":\"loss\"},\"type\":\"log\"},\"width\":1000,\"height\":500,\"title\":{\"text\":\"loss vs. val loss\"},\"xaxis\":{\"title\":{\"text\":\"Epoki\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('1c630b2f-00e8-4073-9e25-8d201a9a27db');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = tokenizer.texts_to_sequences(test_texts)\n",
        "X_test = pad_sequences(sequences, maxlen=maxlen)\n",
        "y_test = np.asarray(test_labels)\n",
        "\n",
        "model.evaluate(X_test, y_test, verbose=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEQsMnHl1iLm",
        "outputId": "5b104539-f311-4325-b148-1eaa87504455"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7247532606124878, 0.818120002746582]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zesndqS117jQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
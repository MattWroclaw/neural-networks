{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8TBfOD4jDUCsTTmHtYAj7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MattWroclaw/neural-networks/blob/main/03_keras/03_udnerfitting_overfitting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Główne problemy uczenia maszynowego: przeuczenie (overfitting) oraz niedouczenie (underfitting)\n",
        "\n",
        ">Celem tego notebook'a jest pokazanie przykładów zbyt dobrego dopasowanie modelu do danych uczących (przeuczenie) oraz zbyt słabego dopasowania modelu do danych uczących (niedouczenie).\n",
        ">\n",
        ">Wykorzystamy zbiór z bilioteki Keras składający się z 50000 recenzji filmów oznaczonych sentymentem: pozytywny/negatywny. Recenzje są wstępnie przetworzone, a każda recenzja jest zakodowana jako sekwencja indeksów słów. Słowa są indeksowane według ogólnej częstotliwości w zbiorze danych. Na przykład liczba 5 oznacza piąte najczęściej pojawiające się słowo w danych. Liczba 0 nie oznacza określonego słowa.\n",
        "\n",
        "### Spis treści\n",
        "1. [Import bibliotek](#a1)\n",
        "2. [Załadowanie i przygotowanie danych](#a2)\n",
        "3. [Budowa modelu bazowego](#a3)\n",
        "4. [Budowa 'mniejszego' modelu](#a4)    \n",
        "5. [Budowa 'większego' modelu](#a5)\n",
        "6. [Porównanie wydajności modeli](#a6)\n",
        "7. [Metody regularyzacji](#a7)"
      ],
      "metadata": {
        "id": "QPUtVtL73PKI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MaPqpZwj3JvS"
      },
      "outputs": [],
      "source": [
        "# Przygotowanie środowiska do pracy z Tensorflow 2.0.\n",
        "# Jeśli otrzymasz błąd podczas instalacji Tensorflow uruchom tę komórkę raz jeszcze.\n",
        "\n",
        "!pip uninstall -y tensorflow\n",
        "!pip install -q tensorflow==2.0.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.datasets.imdb import get_word_index\n",
        "from tensorflow.keras.utils import get_file\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "sns.set()\n",
        "tf.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MlDJ3Ez_3U5A",
        "outputId": "5982fccd-a5c2-4d60-f9bd-e53aad2cbf24"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.17.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <a name='a2'></a> 2. Załadowanie i przygotowanie danych"
      ],
      "metadata": {
        "id": "82MMl5ta3X7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_WORDS = 10000   # 10000 najczęściej pojawiających się słów\n",
        "INDEX_FROM = 3\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=NUM_WORDS, index_from=INDEX_FROM)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOf8fnt03Vca",
        "outputId": "8d1e5825-ba71-4b9b-ed4a-d93c8f26dbc3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'train_data shape: {train_data.shape}')\n",
        "print(f'test_data shape: {test_data.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBRuroww3lK3",
        "outputId": "8c805f85-62e5-4369-b1af-0a575fe4e4ad"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_data shape: (25000,)\n",
            "test_data shape: (25000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VB2OHFSi3ly4",
        "outputId": "bd88f53e-ea65-4f2b-b5ab-2370d7102fff"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_idx = get_word_index()\n",
        "word_to_idx = {k:(v + INDEX_FROM) for k, v in word_to_idx.items()}\n",
        "word_to_idx['<PAD>'] = 0\n",
        "word_to_idx['<START>'] = 1\n",
        "word_to_idx['<UNK>'] = 2\n",
        "word_to_idx['<UNUSED>'] = 3\n",
        "idx_to_word = {v: k for k, v in word_to_idx.items()}\n",
        "list(idx_to_word.items())[:10]\n",
        "print(' '.join(idx_to_word[idx] for idx in train_data[0]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWONu-td3pHM",
        "outputId": "26a31915-0f96-4c8d-ab39-f4b717694568"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "\u001b[1m1641221/1641221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ten kod w Pythonie dotyczy przetwarzania słów na ich indeksy w celu przygotowania danych tekstowych do modelu uczenia maszynowego, np. przy użyciu sekwencji słów w sieciach neuronowych. Wyjaśnię krok po kroku.\n",
        "\n",
        "### Co robi kod?\n",
        "\n",
        "1. **Pobranie indeksów słów**:\n",
        "   ```python\n",
        "   word_to_idx = get_word_index()\n",
        "   ```\n",
        "   - Funkcja `get_word_index()` prawdopodobnie pobiera słownik, w którym kluczem jest słowo, a wartością jego indeks (czyli unikalna liczba przypisana do każdego słowa). Przykładowo, słowo \"apple\" może być mapowane na indeks 25.\n",
        "\n",
        "2. **Modyfikacja indeksów słów**:\n",
        "   ```python\n",
        "   word_to_idx = {k:(v + INDEX_FROM) for k, v in word_to_idx.items()}\n",
        "   ```\n",
        "   - To przekształcenie przesuwa wszystkie wartości indeksów o pewną liczbę `INDEX_FROM`. Załóżmy, że `INDEX_FROM = 3`, wtedy wszystkie indeksy zostaną powiększone o 3.\n",
        "   - Powód tego przesunięcia to często zarezerwowanie początkowych indeksów dla specjalnych tokenów, jak `'<PAD>'`, `'<START>'`, itp.\n",
        "\n",
        "3. **Dodanie specjalnych tokenów**:\n",
        "   ```python\n",
        "   word_to_idx['<PAD>'] = 0\n",
        "   word_to_idx['<START>'] = 1\n",
        "   word_to_idx['<UNK>'] = 2\n",
        "   word_to_idx['<UNUSED>'] = 3\n",
        "   ```\n",
        "   - Tokeny specjalne są dodawane ręcznie do słownika:\n",
        "     - `'<PAD>' = 0`: Reprezentuje padding, używany do wyrównania sekwencji do tej samej długości.\n",
        "     - `'<START>' = 1`: Wskazuje początek sekwencji.\n",
        "     - `'<UNK>' = 2`: Oznacza nieznane słowo (ang. unknown).\n",
        "     - `'<UNUSED>' = 3`: Rezerwowy token, często nieużywany w praktyce, ale zarezerwowany.\n",
        "\n",
        "4. **Odwrócenie słownika**:\n",
        "   ```python\n",
        "   idx_to_word = {v: k for k, v in word_to_idx.items()}\n",
        "   ```\n",
        "   - Tworzony jest odwrotny słownik, który mapuje indeksy na słowa. To pozwala na zamianę indeksów z powrotem na tekst.\n",
        "\n",
        "5. **Podgląd pierwszych 10 elementów słownika**:\n",
        "   ```python\n",
        "   list(idx_to_word.items())[:10]\n",
        "   ```\n",
        "   - Wyświetlana jest pierwsza dziesiątka par (indeks, słowo) z odwróconego słownika, żeby zobaczyć, jak indeksy są przypisane do słów.\n",
        "\n",
        "6. **Wyświetlenie pierwszego zdania w `train_data`**:\n",
        "   ```python\n",
        "   print(' '.join(idx_to_word[idx] for idx in train_data[0]))\n",
        "   ```\n",
        "   - `train_data[0]` to pierwsza sekwencja (zdanie) z danych treningowych, która jest zakodowana jako sekwencja indeksów.\n",
        "   - Każdy indeks jest tłumaczony na odpowiadające mu słowo z pomocą słownika `idx_to_word`, a następnie słowa są łączone w zdanie za pomocą funkcji `' '.join()`.\n",
        "\n",
        "### Przykład działania:\n",
        "Jeśli pierwszy wiersz w `train_data[0]` zawiera indeksy np. `[1, 25, 43, 7, 2]`, kod zamieni te indeksy na słowa:\n",
        "- `1` to `'<START>'`\n",
        "- `25` to np. `\"apple\"`\n",
        "- `43` to np. `\"is\"`\n",
        "- `7` to np. `\"good\"`\n",
        "- `2` to `'<UNK>'` (nieznane słowo)\n",
        "\n",
        "Wynikowy tekst, który zostanie wyświetlony, mógłby wyglądać tak:\n",
        "```\n",
        "<START> apple is good <UNK>\n",
        "```\n",
        "\n",
        "Ten kod pomaga w tłumaczeniu sekwencji indeksów z powrotem na czytelny tekst, co jest użyteczne podczas interpretacji wyników modelu oraz do weryfikacji poprawności przetwarzania danych."
      ],
      "metadata": {
        "id": "GPrfQE8y_ALb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzcZXVtN3thk",
        "outputId": "b8ff9eb8-b1ff-419d-fe45-1e5d8df8f39a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 1, 0, 0, 1, 0, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def multi_hot_sequences(sequences, dimension):\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, word_indices in enumerate(sequences):\n",
        "        results[i, word_indices] = 1.0\n",
        "    return results\n",
        "\n",
        "train_data = multi_hot_sequences(train_data, dimension=NUM_WORDS)\n",
        "test_data = multi_hot_sequences(test_data, dimension=NUM_WORDS)\n",
        "train_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fq-8atrh_EFs",
        "outputId": "bb32d69f-efec-4721-8ac0-8dd49bb542b5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 10000)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ten kod w Pythonie tworzy **\"multi-hot encoding\"** dla sekwencji danych, prawdopodobnie reprezentujących indeksy słów w tekście, np. w ramach przetwarzania języka naturalnego (NLP). Wyjaśnię krok po kroku:\n",
        "\n",
        "### Funkcja `multi_hot_sequences`\n",
        "\n",
        "#### Argumenty:\n",
        "- `sequences`: Lista sekwencji (czyli lista list), gdzie każda podlista zawiera indeksy słów.\n",
        "- `dimension`: Całkowita liczba możliwych słów (rozmiar słownika), czyli ile unikalnych słów może wystąpić w tekście.\n",
        "\n",
        "#### Działanie:\n",
        "1. **Tworzenie macierzy `results`:**\n",
        "   ```python\n",
        "   results = np.zeros((len(sequences), dimension))\n",
        "   ```\n",
        "   - Funkcja tworzy macierz o wymiarach `(liczba sekwencji, liczba możliwych słów)`, czyli np. `(10000, 5000)`, gdzie `10000` to liczba zdań (sekwencji), a `5000` to liczba możliwych słów.\n",
        "   - Na początku macierz jest wypełniona zerami.\n",
        "\n",
        "2. **Iteracja po sekwencjach i wypełnianie macierzy:**\n",
        "   ```python\n",
        "   for i, word_indices in enumerate(sequences):\n",
        "       results[i, word_indices] = 1.0\n",
        "   ```\n",
        "   - `enumerate(sequences)` iteruje po każdej sekwencji w `sequences`.\n",
        "   - `i` to indeks sekwencji, a `word_indices` to lista indeksów słów dla danej sekwencji.\n",
        "   - Dla każdego zdania (sekwencji), funkcja ustawia `1.0` w miejscach odpowiadających indeksom słów w tej sekwencji. W ten sposób dla danego zdania powstaje wektor, który ma wartość `1` tam, gdzie dane słowo występuje, a `0`, gdzie go nie ma.\n",
        "\n",
        "3. **Zwrócenie wyniku:**\n",
        "   ```python\n",
        "   return results\n",
        "   ```\n",
        "   - Zwracana jest macierz `results`, która zawiera multi-hot encoding dla każdej sekwencji.\n",
        "\n",
        "### Wywołanie funkcji:\n",
        "```python\n",
        "train_data = multi_hot_sequences(train_data, dimension=NUM_WORDS)\n",
        "test_data = multi_hot_sequences(test_data, dimension=NUM_WORDS)\n",
        "```\n",
        "- Funkcja `multi_hot_sequences` jest używana, aby zamienić dane treningowe i testowe (sekwencje indeksów słów) na ich odpowiednie **multi-hot encodingi**.\n",
        "- `NUM_WORDS` to liczba słów w słowniku (czyli wartość `dimension`).\n",
        "\n",
        "### Ostatnia linia:\n",
        "```python\n",
        "train_data.shape\n",
        "```\n",
        "- `train_data.shape` zwraca wymiary macierzy `train_data`, co pozwala sprawdzić, ile jest wierszy (sekwencji) i ile kolumn (słów).\n",
        "\n",
        "### Przykład:\n",
        "Jeśli `train_data` wygląda tak:\n",
        "```python\n",
        "train_data = [[1, 3], [2, 4, 5]]\n",
        "dimension = 6\n",
        "```\n",
        "\n",
        "Macierz wynikowa po zakodowaniu będzie wyglądała tak:\n",
        "```\n",
        "[[0, 1, 0, 1, 0, 0],   # W pierwszym zdaniu występują słowa o indeksach 1 i 3\n",
        " [0, 0, 1, 0, 1, 1]]   # W drugim zdaniu występują słowa o indeksach 2, 4 i 5\n",
        "```\n",
        "\n",
        "Każdy wiersz reprezentuje jedno zdanie, a kolumny odpowiadają wszystkim możliwym słowom w słowniku. Kolumna ma wartość `1`, gdy dane słowo występuje w zdaniu, a `0`, gdy nie występuje."
      ],
      "metadata": {
        "id": "CMtbqySx__rU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yf1DGSbBAHZh",
        "outputId": "5caa0902-dd28-4dc4-a115-bff952f09bab"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 10000)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0SP-ba34AIAR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MattWroclaw/neural-networks/blob/main/02_basics/03_entropia_funkcje_straty.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpksqwHQTyN0"
      },
      "source": [
        "* @author: krakowiakpawel9@gmail.com  \n",
        "* @site: e-smartdata.org"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gjFu2l9IMsj"
      },
      "source": [
        "### Funkcje Straty - Loss Function:\n",
        "1. [Entropia rozkładu prawdopodobieństwa](#a0)\n",
        "2. [Binarna entropia krzyżowa - Binary Crossentropy](#a1)\n",
        "3. [Kategoryczna entropia krzyżowa - Categorical Crossentropy](#a0)\n",
        "\n",
        "#### <a name='a0'></a> 1. Entropia rozkładu prawdopodobieństwa\n",
        "$$Entropia = - \\sum_{i}p_{i} * log(p_{i})$$\n",
        "Gdzie $p_{i}$ to prawdopodobieństwo zajścia $i$-tego zdarzenia. Entropia charakteryzuje mozliwośc oddawania informacji przez żródło. Inaczej jest to miara nieokreśloności/niepewności. Średnie zdziwienie (wartość oczekiwana zdziwienia)\n",
        "\n",
        "Pokazuje czystość zbioru: np. czy jest dobrze sklasyfikowana dana grupa.\n",
        "\n",
        "Entropia jest miarą niepewności lub nieokreśloności w zbiorze danych. Im wyższa entropia, tym większa niepewność. Na przykład, jeśli mamy zbiór danych, w którym wszystkie wartości są takie same, to entropia będzie równa zero, ponieważ nie ma żadnej niepewności co do wartości danych. Jeśli jednak mamy zbiór danych, w którym wartości są bardzo różne, to entropia będzie wysoka, ponieważ istnieje duża niepewność co do wartości danych.  \n",
        "\n",
        " W uczeniu maszynowym entropia jest używana jako miara jakości modelu. Na przykład, jeśli mamy model, który przewiduje prawdopodobieństwo wystąpienia określonego zdarzenia, to entropia może być użyta do pomiaru tego, jak dobrze model przewiduje prawdopodobieństwo.  \n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YT-REXq_ghEj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7c45c8b-8bae-4b78-9d0f-48d71c1176b0"
      },
      "source": [
        "# Przygotowanie środowiska do pracy z Tensorflow 2.0.\n",
        "# Jeśli otrzymasz błąd podczas instalacji Tensorflow uruchom tę komórkę raz jeszcze.\n",
        "\n",
        "!pip uninstall -y tensorflow\n",
        "!pip install -q tensorflow==2.0.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.0.0\n",
            "Uninstalling tensorflow-2.0.0:\n",
            "  Successfully uninstalled tensorflow-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVlLr1EBD3nj",
        "outputId": "f2515d30-9d5b-4f93-86ad-b330e548cf0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.17.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyxtFjeeGECa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ca52c10-d1b9-4c7d-f50e-846b0a3cadca"
      },
      "source": [
        "import numpy as np\n",
        "np.set_printoptions(precision=6, suppress=True)\n",
        "\n",
        "def entropy(labels, base=None):\n",
        "    from math import log, e\n",
        "    n_labels = len(labels)\n",
        "\n",
        "    if n_labels <= 1:\n",
        "        return 0\n",
        "\n",
        "    value, counts = np.unique(labels, return_counts=True)\n",
        "    probs = counts / n_labels\n",
        "    n_classes = np.count_nonzero(probs)\n",
        "\n",
        "    if n_classes <= 1:\n",
        "        return 0\n",
        "\n",
        "    ent = 0.\n",
        "\n",
        "    base = e if base is None else base\n",
        "    for i in probs:\n",
        "        ent -= i * log(i, base)\n",
        "    return ent\n",
        "\n",
        "\n",
        "labels = [1,3,5,2,3,5,3,2,1,3,4,5]\n",
        "entropy(labels)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.5171063970610277"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_7m4M8MZkHp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b22f379-5391-47bf-ea45-e0ee4e019375"
      },
      "source": [
        "labels = [3, 3, 3, 3, 3, 3, 3]\n",
        "entropy(labels)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "poJ2iHokV4jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cce9af34-a404-40c4-c128-612987d4c4d9",
        "id": "L7fNu8b2V4_m"
      },
      "source": [
        "labels = [300, 3, 3, 3, 3, 3, 3]\n",
        "entropy(labels)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.410116318288409"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [30000000, 3, 3, 3, 3, 3, 3]\n",
        "entropy(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvuo-E7iYDVn",
        "outputId": "6ef1e07c-e918-473a-8228-42129cc20727"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.410116318288409"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyRgL5ybaV7N"
      },
      "source": [
        "#### <a name='a1'></a> 2. Binarna entropia krzyżowa - Binary Crossentropy\n",
        "$$Binary\\ CrossEntropy = - \\frac{1}{N}\\sum_{i=1}^{N}(y_{i}*log(p(y_{i})) + (1-y_{i}) * log(1-p(y_{i}))$$  \n",
        "\n",
        "**To jest dla klas binarnych.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXA1CKEAbs6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c85c2c97-4a13-4ca6-fadc-049a40c7d351"
      },
      "source": [
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "\n",
        "y_true = np.array([1, 0, 1, 1, 0, 1], dtype='float')\n",
        "y_pred = np.array([0, 0, 1, 1, 0, 1], dtype='float')\n",
        "\n",
        "binary_crossentropy(y_true, y_pred)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float64, numpy=2.6863493584930573>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m3HObz42WjWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28f92620-5076-4037-d96f-e4542e13732e",
        "id": "9f1GZY_jWkT1"
      },
      "source": [
        "#  teraz przekonajmy się co będzie jak wszystko będzie się zgadzać\n",
        "\n",
        "y_true = np.array([1, 0, 1, 1, 0, 1], dtype='float')\n",
        "y_pred = np.array([1, 0, 1, 1, 0, 1], dtype='float')\n",
        "\n",
        "binary_crossentropy(y_true, y_pred)\n",
        "\n",
        "#  Dążymy do minimalizacji entropii"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float64, numpy=1.0000000494736474e-07>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EK-uYzfca0tG"
      },
      "source": [
        "#### <a name='a2'></a> 3. Kategoryczna entropia krzyżowa - Categorical Crossentropy\n",
        "$$Categorical\\ CrossEntropy= - \\sum_{i}y_{i} * log(p(y_{i})) $$\n",
        "\n",
        "**To jest dla wieloklasowych**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXCV2LIlanLc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c0e4db7-5a2a-4a2c-df5b-7ae28f38495d"
      },
      "source": [
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "\n",
        "y_true = np.array([1, 0, 1, 1, 2, 0, 1, 1, 2], dtype='float')\n",
        "y_pred = np.array([0, 0, 1, 1, 2, 0, 1, 2, 2], dtype='float')\n",
        "\n",
        "categorical_crossentropy(y_true, y_pred)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float64, numpy=30.23015636684835>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16rmIAP2hwN3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a0b3cab-fb4c-4cdd-fed9-e9a8a5730c53"
      },
      "source": [
        "y_true = np.array([0, 0, 1, 1, 2, 0, 1, 1, 2], dtype='float')\n",
        "y_pred = np.array([0, 0, 1, 1, 2, 0, 1, 1, 2], dtype='float')\n",
        "\n",
        "categorical_crossentropy(y_true, y_pred)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float64, numpy=13.862943611198904>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}